---
title: "p8105_hw6_hz2710"
author: "Huili Zheng"
date: "12/4/2021"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
theme_set(theme_gray(base_size = 10) + theme(legend.position = "bottom"))
```

# Problem 1

Firstly, we load and wrangle the data.

```{r}
birthweight_df = 
  read_csv("./data/birthweight.csv") %>%
  mutate(
    babysex = factor(babysex, levels = c("1", "2")),
    frace = factor(frace, levels = c("1", "2", "3", "4", "8", "9")),
    malform = factor(malform, levels = c("0", "1")),
    mrace = factor(mrace, levels = c("1","2","3","4","8"))
  ) 
```

There is `r sum(is.na(birthweight_df))` missing value.

Then we use the **stepwise regression method** to select the model.

```{r}
model_fit = lm(bwt ~ ., data = birthweight_df) %>%
  step(direction = "backward")

summary(model_fit)
```

**Description of modeling process:** The model was selected by backward elimination method. It started with all candidate variables and then removed the predictor with the p-value higher than then criterion. The model repeated the process until no other variables can be removed without a statistically significant loss of fit.

A plot of model residuals against fitted values:

```{r}
birthweight_df %>%
  add_predictions(model_fit) %>%
  add_residuals(model_fit) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .4, size = 2) +
    labs(x = "Fitted value", y = "Residuals", title = "Residuals against fitted values")
```

The residuals bounce around 0 and form a horizontal band around 0 but with some outliers on small fitted values.

Compare with two models:

Using length at birth and gestational age as predictors (main effects only):

```{r}
model_fit2 = lm(bwt ~ blength + gaweeks, data = birthweight_df)
 
model_fit2 %>% broom::tidy() %>% knitr::kable()
```

Model using head circumference, length, sex, and all interactions (including the three-way interaction) between these:

```{r}
model_fit3 = lm(bwt ~ bhead * blength * babysex, data = birthweight_df)

model_fit3 %>% broom::tidy() %>% knitr::kable()
```

Make comparisons in terms of the cross-validated prediction error.

```{r}
cv_df = 
  crossv_mc(birthweight_df, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>%
  mutate(
    model1 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    model2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model3 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>%
  mutate(
    rmse1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
    rmse3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y))
  )
```

```{r}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

We can see that in cross validation model1 got the lowest rmse and model2 got the highest rmse. Model1 had the best performance among these three models while the model2 predicted the worse result.

# Problem 2

