---
title: "p8105_hw6_hz2710"
author: "Huili Zheng"
date: "12/4/2021"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
theme_set(theme_gray(base_size = 10) + theme(legend.position = "bottom"))
```

# Problem 1

Firstly, we load and wrangle the data.

```{r}
birthweight_df = 
  read_csv("./data/birthweight.csv") %>%
  mutate(
    babysex = factor(babysex, levels = c("1", "2")),
    frace = factor(frace, levels = c("1", "2", "3", "4", "8", "9")),
    malform = factor(malform, levels = c("0", "1")),
    mrace = factor(mrace, levels = c("1","2","3","4","8"))
  ) 
```

There is `r sum(is.na(birthweight_df))` missing value.

Then we use the **stepwise regression method** to select the model.

```{r}
model_fit = lm(bwt ~ ., data = birthweight_df) %>%
  step(direction = "backward")

summary(model_fit)
```

**Description of modeling process:** The model was selected by backward elimination method. It started with all candidate variables and then removed the predictor with the p-value higher than then criterion. The model repeated the process until no other variables can be removed without a statistically significant loss of fit.

A plot of model residuals against fitted values:

```{r}
birthweight_df %>%
  add_predictions(model_fit) %>%
  add_residuals(model_fit) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .4, size = 2) +
    labs(x = "Fitted value", y = "Residuals", title = "Residuals against fitted values")
```

The residuals bounce around 0 and form a horizontal band around 0 but with some outliers on small fitted values.

Compare with two models:

Using length at birth and gestational age as predictors (main effects only):

```{r}
model_fit2 = lm(bwt ~ blength + gaweeks, data = birthweight_df)
 
model_fit2 %>% broom::tidy() %>% knitr::kable()
```

Model using head circumference, length, sex, and all interactions (including the three-way interaction) between these:

```{r}
model_fit3 = lm(bwt ~ bhead * blength * babysex, data = birthweight_df)

model_fit3 %>% broom::tidy() %>% knitr::kable()
```

Make comparisons in terms of the cross-validated prediction error.

```{r}
cv_df = 
  crossv_mc(birthweight_df, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>%
  mutate(
    model1 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    model2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model3 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>%
  mutate(
    rmse1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
    rmse3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y))
  )
```

```{r}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

We can see that in cross validation model1 got the lowest rmse and model2 got the highest rmse. Model1 had the best performance among these three models while the model2 predicted the worse result.

# Problem 2

Import the data:

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
bootStraps = 
  weather_df %>%
  bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    results_glance = map(models, broom::glance)) %>%
  select(-strap, -models) %>%
  unnest(results, results_glance) %>%
  select(.id, term, estimate, r.squared) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>%
  janitor::clean_names() %>%
  mutate(log_coef = log(intercept * tmin))
bootStraps
```

Plot the distribution of r̂2:

```{r}
bootStraps %>%
  ggplot(aes(x = r_squared)) +
  geom_density() +
  labs(x = "Estimated r squared", title = "Distribution of r̂2")
```

The r̂2 followed a normal distribution and had a well bell shaped with a mean value around 0.91. It shows that the model had a good performance.

Plot the distribution of log(β̂0 ∗β1):
```{r}
bootStraps %>%
  ggplot(aes(x = log_coef)) +
  geom_density() +
  labs(x = "log(beta0 x beta1)", title = "Distribution of log_coef")
```

The log(β̂0 ∗β1) followed a normal distribution and had a bell shaped with a mean value around 2.02.

## 2.5% and 97.5% quantiles for \hat{r}^2:

```{r}
CI_r = bootStraps %>%
  pull(r_squared) %>%
  quantile(c(0.025, 0.975))

CI_r
```

The 95% confidence interval for  r̂2 is (`r CI_r[[1]]`, `r CI_r[[2]]`).

## 2.5% and 97.5% quantiles for r̂2:

```{r}
CI_log = bootStraps %>%
  pull(log_coef) %>%
  quantile(c(0.025, 0.975))

CI_log
```

The 95% confidence interval for  log(β̂0 ∗β1) is (`r CI_log[[1]]`, `r CI_log[[2]]`).
